# -*- coding: utf-8 -*-
"""Sentiment Analysis ML Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14MfISize5sBZbg8ZV5BYhB3XRqt17dcJ

###**Machine Learning** Mini Project

#Aim: 
To predict the rating of an application using sentiment analysis of the past reviews and ratings of different applications of play store.

We check for the ratings of the application on the basis of different criteria and thus understand on how every criteria contributes to the prediction of ratings.

Importing Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import joblib
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
from sklearn.metrics import precision_score,recall_score
from sklearn.metrics import f1_score
from sklearn.model_selection import cross_val_predict
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import roc_curve
from sklearn.naive_bayes import MultinomialNB
import warnings
warnings.filterwarnings("ignore")

"""Reading Data-Sets

Link for the Dataset:

https://www.kaggle.com/datasets/geothomas/playstore-dataset
"""

df=pd.read_csv("Playstore_final.csv")

"""Link for the Dataset:

https://www.kaggle.com/datasets/rowemorehouse/googleplaystoreuserreviews
"""

text_data = pd.read_csv('googleplaystore_user_reviews.csv')

"""##Gaussian Naive Bayes

**Gaussian Naive Bayes** is a variant of Naive Bayes that follows Gaussian normal distribution and supports continuous data.
"""

df.shape #Shape of the Dataset in form(Rows, Columns)

df.head()

#Converting String data to Numeric Values (0 for True and 1 for False)
df['In app purchases']=np.where(df['In app purchases']=='True',0,1)
df['Ad Supported']=np.where(df['Ad Supported']=='True',0,1)
df['Editor Choice']=np.where(df['Editor Choice']=='True',0,1)
df['Free']=np.where(df['Free']=='True',0,1)

#Segregrating X Y data and picking only required data
data=df[['Rating','Minimum Installs','Free','Price','In app purchases','Ad Supported','Editor Choice','Reviews']]

#Converting Rating and Reviews to binary values considering their respective ranges.
data['Rating'] = data['Rating'].apply(lambda Rating : +1 if Rating > 3 else 0)
data['Reviews'] = data['Reviews'].apply(lambda Reviews : +1 if Reviews > 100 else 0)

data.head()

data=data.dropna(axis=0,how='any')  #features having missing value/s are removed.
data.shape

#Splitting the data into train and test
x=data[['Minimum Installs','Free','Price','In app purchases','Ad Supported','Editor Choice','Reviews']]
y=data['Rating']


xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.6,random_state=1)

xtrain.shape

xtest.shape

ytrain.shape

ytest.shape

#Building Naive Bayes Model

gnb=GaussianNB()

# Fit the model with training set
gnb.fit(xtrain, ytrain)

#Making predictions with the model
ypred=gnb.predict(xtest)

#Evaluating the performance

qq= round(metrics.accuracy_score(ytest,ypred),2)*100
print(qq,'%')

#Plotting the confusion matrix
confusion_matrix=pd.crosstab(ytest,ypred,rownames=['Actual'],colnames=[' Predicted'])

#Plotting the Confusion Matrix
sns.heatmap(confusion_matrix,annot=True)

#Numeric Values for Confusion Matrix
from sklearn.metrics import confusion_matrix
confusion_matrix(ytest,ypred)

q1 = round(precision_score(ytest,ypred),2)*100 #Precision Score
print(q1,"%")

q2 = round(recall_score(ytest,ypred),2)*100 #Recall Score
print(q2,"%")

q3 = round(f1_score(ytest,ypred),2)*100 #F1 Score
print(q3,"%")

#ROC Curve

y_scores=cross_val_predict(gnb,xtrain,ytrain,cv=3,method='predict')


fpr,tpr,thresh=roc_curve(ytrain,y_scores)

def plot_roc(fpr,tpr,label=None):
  plt.plot(fpr,tpr,linewidth=2,label=label)
  plt.plot([0,1],[0,1],"k--")
  plt.axis([0,1,0,1])
  plt.xlabel('False Positive Curve')
  plt.ylabel('True Positive Curve')
                         
plot_roc(fpr,tpr)

"""##Multinomial Naive Bayes

The Multinomial Naive Bayes algorithm is a Bayesian learning approach popular in Natural Language Processing (NLP). The program guesses the tag of a text, such as an email or a newspaper story, using the Bayes theorem. It calculates each tag's likelihood for a given sample and outputs the tag with the greatest chance.
"""

text_data.head()

text_data.shape

# Convert text to lowercase
text_data['Translated_Review'] = text_data['Translated_Review'].str.strip().str.lower()

text_data=text_data.dropna(axis=0,how='any')  #features having missing value/s are removed.

text_data['Sentiment'] = np.where(text_data['Sentiment']=='Positive',1,0) #Sentiment to Binary Values

text_data.head()

# Split into training and testing data
text_x = text_data['Translated_Review']
text_y = text_data['Sentiment']
text_x, text_x_test, text_y, text_y_test = train_test_split(text_x,text_y, stratify=text_y, test_size=0.5, random_state=42)

text_x.shape

text_x_test.shape

text_y.shape

text_y_test.shape

"""**Vectorization:** To make sense of this data for our machine learning algorithm, we will need to convert each review to a numerical representation that we call vectorization."""

#Vectorize text reviews to numbers
vec = CountVectorizer(stop_words='english')
text_x = vec.fit_transform(text_x).toarray()
text_x_test = vec.transform(text_x_test).toarray()

model = MultinomialNB() #Running the Model
model.fit(text_x, text_y)

text_ypred=model.predict(text_x_test) #Predicting Values

tt = round(model.score(text_x_test,text_y_test),2)*100 #Score
print(tt,"%")

#Plotting the confusion matrix
text_confusion_matrix=pd.crosstab(text_y_test,text_ypred,rownames=['Actual'],colnames=[' Predicted'])

#Plotting the Confusion Matrix
sns.heatmap(text_confusion_matrix,annot=True)

#Numeric Values for Confusion Matrix
from sklearn.metrics import confusion_matrix
confusion_matrix(text_y_test,text_ypred)

t1 = round(precision_score(text_y_test,text_ypred),2)*100 #Prescision Score
print(t1,'%')

t2 = round(recall_score(text_y_test,text_ypred),2)*100 #Recall Score
print(t2,'%')

t3 = round(f1_score(text_y_test,text_ypred),2)*100 #F1 Score
print(t3,'%')

#ROC Curve

text_y_scores=cross_val_predict(model,text_x,text_y,cv=3,method='predict')


fpr,tpr,thresh=roc_curve(text_y,text_y_scores)

def plot_roc(fpr,tpr,label=None):
  plt.plot(fpr,tpr,linewidth=2,label=label)
  plt.plot([0,1],[0,1],"k--")
  plt.axis([0,1,0,1])
  plt.xlabel('False Positive Curve')
  plt.ylabel('True Positive Curve')
                         
plot_roc(fpr,tpr)

"""Prediction on the basis of other categories."""

test_case = np.ones((1,7), dtype = np.float64) 
test_case[0][0] = int(input("Minimum Installs: "))
#Enter 1 for True and 0 for False
test_case[0][1] = int(input("Free: "))
if test_case[0][1] == 0:
  test_case[0][2] == int(input("Price: "))
else:
  test_case[0][2] == 0
test_case[0][3] = int(input("In app Purchases: "))
test_case[0][4] = int(input("Ad Supported: "))
test_case[0][5] = int(input("Editor's Choice: "))
test_case[0][6] = int(input("Reviews: "))

b = gnb.predict(test_case)

if b == 1:
  print("Positive Review")
else:
  print("Negative Review")

"""Prediction on the basis of review."""

review = input("Enter your Review: ")
a = model.predict(vec.transform([review]))

if a == 1:
  print("Positive Review")
else:
  print("Negative Review")

"""Comparison between the two models."""

from tabulate import tabulate
 
mydata = [
          ["Accuracy",qq, tt],
          ["Precision",q1, t1],
          ["Recall",q2, t2],
          ["F1",q3, t3]]
 
# create header
head = ["Title","GaussianNB", "MultinomialNB"]
 
# display table
print(tabulate(mydata, headers=head, tablefmt="grid"))

"""#Conclusion:
After running both the models we notice that the score for MultinomialNB is greater than GaussianNB this proves that the text ratings given by users have a greater effect on the Ratings of an application. The ROC curve and confusion matrix help in proving the same.
"""

! jupyter nbconvert --to html /content/Sentiment_Analysis_ML_Project.ipynb